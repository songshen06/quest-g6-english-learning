#!/usr/bin/env python3
"""
é€šç”¨ Quests ç”Ÿæˆè„šæœ¬
æ ¹æ®æ¯ä¸ªå•å…ƒçš„wordsã€phrasesã€patternsè‡ªåŠ¨ç”Ÿæˆæ ‡å‡†æ ¼å¼çš„questså†…å®¹

æ”¯æŒä»»æ„å¹´çº§å’Œä¸åŒæ–‡ä»¶ç»“æ„ï¼š
- æ”¯æŒæ—§æ ¼å¼ï¼ˆwordsä¸ºå­—å…¸åˆ—è¡¨ï¼‰
- æ”¯æŒæ–°æ ¼å¼ï¼ˆwordsåŒ…å«idã€enã€zhç­‰å­—æ®µï¼‰
- è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç»“æ„å¹¶é€‚é…

ç”Ÿæˆçš„questsåŒ…å«4ç§ç±»å‹ï¼š
1. vocabulary-matching - è¯è¯­é…å¯¹ç»ƒä¹ 
2. sentence-sorting - è¯è¯­æ’åºç»ƒä¹ 
3. en-to-zh - è‹±ç¿»ä¸­ç»ƒä¹ 
4. zh-to-en - ä¸­ç¿»è‹±ç»ƒä¹ 
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class GenericQuestGenerator:
    """é€šç”¨Questå†…å®¹ç”Ÿæˆå™¨"""

    def __init__(self, content_dir: str):
        self.content_dir = Path(content_dir)

    def normalize_word_format(self, word_data: Any) -> Optional[Dict[str, str]]:
        """ç»Ÿä¸€ä¸åŒæ ¼å¼çš„wordæ•°æ®"""
        if isinstance(word_data, dict):
            # æ–°æ ¼å¼ï¼šåŒ…å«idå­—æ®µ
            if 'id' in word_data and 'en' in word_data and 'zh' in word_data:
                return {
                    'id': word_data['id'],
                    'en': word_data['en'],
                    'zh': word_data['zh']
                }
            # æ—§æ ¼å¼ï¼šç›´æ¥åŒ…å«enå’Œzh
            elif 'en' in word_data and 'zh' in word_data:
                return {
                    'id': str(hash(word_data['en']))[:8],
                    'en': word_data['en'],
                    'zh': word_data['zh']
                }
        return None

    def normalize_phrase_format(self, phrase_data: Any) -> Optional[Dict[str, str]]:
        """ç»Ÿä¸€ä¸åŒæ ¼å¼çš„phraseæ•°æ®"""
        if isinstance(phrase_data, dict):
            # æ–°æ ¼å¼ï¼šåŒ…å«idå­—æ®µ
            if 'id' in phrase_data and 'en' in phrase_data and 'zh' in phrase_data:
                return {
                    'id': phrase_data['id'],
                    'en': phrase_data['en'],
                    'zh': phrase_data['zh']
                }
            # æ—§æ ¼å¼ï¼šç›´æ¥åŒ…å«enå’Œzh
            elif 'en' in phrase_data and 'zh' in phrase_data:
                return {
                    'id': str(hash(phrase_data['en']))[:8],
                    'en': phrase_data['en'],
                    'zh': phrase_data['zh']
                }
        return None

    def normalize_pattern_format(self, pattern_data: Any) -> Optional[Dict[str, str]]:
        """ç»Ÿä¸€ä¸åŒæ ¼å¼çš„patternæ•°æ®"""
        if isinstance(pattern_data, dict):
            # æ–°æ ¼å¼ï¼šåŒ…å«qå’Œaå­—æ®µ
            if 'q' in pattern_data and 'a' in pattern_data:
                return {
                    'q': pattern_data['q'],
                    'a': pattern_data['a']
                }
            # å…¼å®¹å…¶ä»–å¯èƒ½çš„æ ¼å¼
            elif 'question' in pattern_data and 'answer' in pattern_data:
                return {
                    'q': pattern_data['question'],
                    'a': pattern_data['answer']
                }
            elif 'example' in pattern_data and 'translation' in pattern_data:
                return {
                    'q': pattern_data['example'],
                    'a': pattern_data['translation']
                }
        return None

    def extract_words(self, module_data: Dict) -> List[Dict[str, str]]:
        """æå–å¹¶æ ‡å‡†åŒ–wordsæ•°æ®"""
        words_raw = module_data.get('words', [])
        words = []

        for word_data in words_raw:
            normalized = self.normalize_word_format(word_data)
            if normalized:
                words.append(normalized)

        logger.debug(f"æå–åˆ° {len(words)} ä¸ªwords")
        return words

    def extract_phrases(self, module_data: Dict) -> List[Dict[str, str]]:
        """æå–å¹¶æ ‡å‡†åŒ–phrasesæ•°æ®"""
        phrases_raw = module_data.get('phrases', [])
        phrases = []

        for phrase_data in phrases_raw:
            normalized = self.normalize_phrase_format(phrase_data)
            if normalized:
                phrases.append(normalized)

        logger.debug(f"æå–åˆ° {len(phrases)} ä¸ªphrases")
        return phrases

    def extract_patterns(self, module_data: Dict) -> List[Dict[str, str]]:
        """æå–å¹¶æ ‡å‡†åŒ–patternsæ•°æ®"""
        patterns_raw = module_data.get('patterns', [])
        patterns = []

        for pattern_data in patterns_raw:
            normalized = self.normalize_pattern_format(pattern_data)
            if normalized:
                patterns.append(normalized)

        logger.debug(f"æå–åˆ° {len(patterns)} ä¸ªpatterns")
        return patterns

    def generate_filename_from_text(self, text: str) -> str:
        """ä»æ–‡æœ¬ç”Ÿæˆæ–‡ä»¶å"""
        filename = text.lower()
        filename = re.sub(r'[^\w\s-]', '', filename)
        filename = re.sub(r'\s+', '-', filename)
        filename = re.sub(r'-+', '-', filename)
        filename = filename.strip('-')
        return filename + ".mp3"

    def create_vocabulary_matching_quest(self, words: List[Dict], phrases: List[Dict]) -> Dict:
        """åˆ›å»ºè¯è¯­é…å¯¹ç»ƒä¹ """
        # åˆå¹¶wordså’Œphrasesä½œä¸ºé…å¯¹å†…å®¹
        all_pairs = []

        # æ·»åŠ å•è¯é…å¯¹ (æœ€å¤š6ä¸ª)
        for word in words[:6]:
            all_pairs.append({
                "en": word['en'],
                "zh": word['zh']
            })

        # æ·»åŠ çŸ­è¯­é…å¯¹ (å¦‚æœå•è¯ä¸è¶³6ä¸ªï¼Œç”¨çŸ­è¯­è¡¥å……)
        for phrase in phrases[:6-len(all_pairs)]:
            all_pairs.append({
                "en": phrase['en'],
                "zh": phrase['zh']
            })

        if len(all_pairs) < 2:
            logger.warning("è¯æ±‡é…å¯¹å†…å®¹ä¸è¶³ï¼Œè‡³å°‘éœ€è¦2ä¸ªè¯æ±‡")
            return None

        # æ·»åŠ å¹²æ‰°é¡¹
        options = []
        remaining_words = words[6:]
        if len(remaining_words) >= 2:
            for option_word in remaining_words[:2]:
                options.append({
                    "en": option_word['en'],
                    "zh": option_word['zh']
                })
        elif len(all_pairs) >= 4:
            # å¦‚æœå‰©ä½™å•è¯ä¸è¶³ï¼Œä»æ­£ç¡®ç­”æ¡ˆä¸­é€‰ä¸€äº›ä½œä¸ºå¹²æ‰°é¡¹
            for i in range(min(2, len(all_pairs))):
                options.append({
                    "en": all_pairs[i]['zh'],
                    "zh": all_pairs[i]['en']
                })

        step = {
            "type": "wordmatching",
            "text": "å°†è‹±è¯­å•è¯ä¸ä¸­æ–‡æ„æ€é…å¯¹",
            "pairs": all_pairs,
            "options": options
        }

        return {
            "id": "vocabulary-matching",
            "title": "è¯è¯­é…å¯¹ç»ƒä¹ ",
            "steps": [step],
            "reward": {"badge": f"/images/rewards/badge-vocab.png", "xp": 10}
        }

    def create_sentence_sorting_quest(self, phrases: List[Dict], patterns: List[Dict]) -> Dict:
        """åˆ›å»ºè¯è¯­æ’åºç»ƒä¹ """
        steps = []

        # ä¼˜å…ˆä½¿ç”¨çŸ­è¯­ï¼Œä¸è¶³æ—¶ä½¿ç”¨patterns
        sorting_items = []

        # æ·»åŠ çŸ­è¯­ (æœ€å¤š3ä¸ª)
        for phrase in phrases[:3]:
            text = phrase['en']
            words = text.split()
            if len(words) >= 3:
                # æ‰“ä¹±å•è¯é¡ºåº
                scrambled = words[1:] + [words[0]]  # ç®€å•æ‰“ä¹±
                filename = self.generate_filename_from_text(text)
                audio_path = f"/audio/tts/{filename}"

                sorting_items.append({
                    "type": "sentencesorting",
                    "text": "å¬å¥å­å¹¶æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—å•è¯",
                    "audio": audio_path,
                    "scrambled": scrambled,
                    "correct": words
                })

        # æ·»åŠ patterns (å¦‚æœçŸ­è¯­ä¸è¶³)
        if len(sorting_items) < 3:
            for pattern in patterns[:3-len(sorting_items)]:
                text = pattern['q']
                words = text.split()
                if len(words) >= 3:
                    scrambled = words[1:] + [words[0]]
                    filename = self.generate_filename_from_text(text)
                    audio_path = f"/audio/tts/{filename}"

                    sorting_items.append({
                        "type": "sentencesorting",
                        "text": "å¬å¥å­å¹¶æŒ‰æ­£ç¡®é¡ºåºæ’åˆ—å•è¯",
                        "audio": audio_path,
                        "scrambled": scrambled,
                        "correct": words
                    })

        if not sorting_items:
            logger.warning("å¥å­æ’åºå†…å®¹ä¸è¶³")
            return None

        return {
            "id": "sentence-sorting",
            "title": "å¥å­æ’åºç»ƒä¹ ",
            "steps": sorting_items,
            "reward": {"badge": f"/images/rewards/badge-sentence.png", "xp": 15}
        }

    def create_en_to_zh_quest(self, phrases: List[Dict], patterns: List[Dict]) -> Dict:
        """åˆ›å»ºè‹±ç¿»ä¸­ç»ƒä¹ """
        steps = []

        # ä¼˜å…ˆä½¿ç”¨çŸ­è¯­
        translation_items = []

        for phrase in phrases[:2]:
            chinese_chars = list(phrase['zh'])
            # é™ä½è¦æ±‚ï¼Œä¸­æ–‡è‡³å°‘2ä¸ªå­—ç¬¦å³å¯
            if len(chinese_chars) >= 2:
                # ç®€å•æ‰“ä¹±ï¼šå°†ç¬¬ä¸€ä¸ªå­—ç¬¦ç§»åˆ°æœ€å
                scrambled = chinese_chars[1:] + [chinese_chars[0]]
                translation_items.append({
                    "type": "entozh",
                    "text": "å°†è‹±è¯­å¥å­ç¿»è¯‘æˆæ­£ç¡®çš„ä¸­æ–‡é¡ºåº",
                    "english": phrase['en'],
                    "scrambledChinese": scrambled,
                    "correctChinese": chinese_chars
                })

        # å¦‚æœçŸ­è¯­ä¸è¶³ï¼Œä½¿ç”¨patterns
        if len(translation_items) < 2:
            for pattern in patterns[:2-len(translation_items)]:
                chinese_chars = list(pattern['a'])
                if len(chinese_chars) >= 2:
                    scrambled = chinese_chars[1:] + [chinese_chars[0]]
                    translation_items.append({
                        "type": "entozh",
                        "text": "å°†è‹±è¯­å¥å­ç¿»è¯‘æˆæ­£ç¡®çš„ä¸­æ–‡é¡ºåº",
                        "english": pattern['q'],
                        "scrambledChinese": scrambled,
                        "correctChinese": chinese_chars
                    })

        if not translation_items:
            logger.warning("è‹±ç¿»ä¸­ç»ƒä¹ å†…å®¹ä¸è¶³")
            return None

        return {
            "id": "en-to-zh",
            "title": "è‹±ç¿»ä¸­ç»ƒä¹ ",
            "steps": translation_items,
            "reward": {"badge": f"/images/rewards/badge-translate.png", "xp": 12}
        }

    def create_zh_to_en_quest(self, phrases: List[Dict], patterns: List[Dict]) -> Dict:
        """åˆ›å»ºä¸­ç¿»è‹±ç»ƒä¹ """
        steps = []

        # ä¼˜å…ˆä½¿ç”¨çŸ­è¯­
        translation_items = []

        for phrase in phrases[:2]:
            english_words = phrase['en'].split()
            # é™ä½è¦æ±‚ï¼Œè‹±æ–‡è‡³å°‘2ä¸ªå•è¯å³å¯
            if len(english_words) >= 2:
                translation_items.append({
                    "type": "zhtoen",
                    "text": "å°†ä¸­æ–‡å¥å­ç¿»è¯‘æˆæ­£ç¡®çš„è‹±æ–‡å•è¯é¡ºåº",
                    "chinese": phrase['zh'],
                    "scrambledEnglish": english_words[1:] + [english_words[0]],
                    "correctEnglish": english_words
                })

        # å¦‚æœçŸ­è¯­ä¸è¶³ï¼Œä½¿ç”¨patterns
        if len(translation_items) < 2:
            for pattern in patterns[:2-len(translation_items)]:
                english_words = pattern['q'].split()
                if len(english_words) >= 2:
                    translation_items.append({
                        "type": "zhtoen",
                        "text": "å°†ä¸­æ–‡å¥å­ç¿»è¯‘æˆæ­£ç¡®çš„è‹±æ–‡å•è¯é¡ºåº",
                        "chinese": pattern['a'],
                        "scrambledEnglish": english_words[1:] + [english_words[0]],
                        "correctEnglish": english_words
                    })

        if not translation_items:
            logger.warning("ä¸­ç¿»è‹±ç»ƒä¹ å†…å®¹ä¸è¶³")
            return None

        return {
            "id": "zh-to-en",
            "title": "ä¸­ç¿»è‹±ç»ƒä¹ ",
            "steps": translation_items,
            "reward": {"badge": f"/images/rewards/badge-language.png", "xp": 12}
        }

    def generate_quests_for_module(self, module_data: Dict) -> List[Dict]:
        """ä¸ºå•ä¸ªæ¨¡å—ç”Ÿæˆquests"""
        words = self.extract_words(module_data)
        phrases = self.extract_phrases(module_data)
        patterns = self.extract_patterns(module_data)

        quests = []

        # 1. è¯è¯­é…å¯¹ç»ƒä¹ 
        vocab_quest = self.create_vocabulary_matching_quest(words, phrases)
        if vocab_quest:
            quests.append(vocab_quest)

        # 2. è¯è¯­æ’åºç»ƒä¹ 
        sorting_quest = self.create_sentence_sorting_quest(phrases, patterns)
        if sorting_quest:
            quests.append(sorting_quest)

        # 3. è‹±ç¿»ä¸­ç»ƒä¹ 
        en_to_zh_quest = self.create_en_to_zh_quest(phrases, patterns)
        if en_to_zh_quest:
            quests.append(en_to_zh_quest)

        # 4. ä¸­ç¿»è‹±ç»ƒä¹ 
        zh_to_en_quest = self.create_zh_to_en_quest(phrases, patterns)
        if zh_to_en_quest:
            quests.append(zh_to_en_quest)

        return quests

    def update_modules_by_pattern(self, pattern: str, dry_run: bool = False) -> Tuple[List[str], List[str]]:
        """æ ¹æ®æ–‡ä»¶åæ¨¡å¼æ›´æ–°æ¨¡å—"""
        files = list(self.content_dir.glob(f"{pattern}.json"))

        updated_files = []
        skipped_files = []

        for file_path in files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    module_data = json.load(f)

                # ç”Ÿæˆæ–°çš„quests
                new_quests = self.generate_quests_for_module(module_data)

                if new_quests:
                    if not dry_run:
                        # å¤‡ä»½åŸæ–‡ä»¶
                        backup_path = file_path.with_suffix('.json.backup')
                        with open(backup_path, 'w', encoding='utf-8') as f:
                            json.dump(module_data, f, ensure_ascii=False, indent=2)

                        # æ›´æ–°quests
                        module_data['quests'] = new_quests

                        # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
                        with open(file_path, 'w', encoding='utf-8') as f:
                            json.dump(module_data, f, ensure_ascii=False, indent=2)

                    updated_files.append(file_path.name)
                    logger.info(f"âœ… {'é¢„è§ˆ' if dry_run else 'æ›´æ–°'}å®Œæˆ: {file_path.name} (ç”Ÿæˆ {len(new_quests)} ä¸ªquests)")
                else:
                    skipped_files.append(file_path.name)
                    logger.warning(f"âš ï¸ è·³è¿‡: {file_path.name} (å†…å®¹ä¸è¶³)")

            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¤±è´¥ {file_path.name}: {e}")

        return updated_files, skipped_files

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="é€šç”¨Questsç”Ÿæˆå™¨")
    parser.add_argument("--content-dir", default="src/content", help="å†…å®¹ç›®å½•è·¯å¾„")
    parser.add_argument("--grade", help="æŒ‡å®šå¹´çº§ (å¦‚: grade3, grade4)")
    parser.add_argument("--pattern", help="è‡ªå®šä¹‰æ–‡ä»¶åæ¨¡å¼ (å¦‚: grade3-lower-*)")
    parser.add_argument("--dry-run", action="store_true", help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…ä¿®æ”¹æ–‡ä»¶")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    content_dir = Path(args.content_dir)
    if not content_dir.exists():
        logger.error(f"âŒ å†…å®¹ç›®å½•ä¸å­˜åœ¨: {content_dir}")
        return 1

    # ç¡®å®šå¤„ç†æ¨¡å¼
    if args.pattern:
        pattern = args.pattern
        description = f"æ¨¡å¼ '{pattern}'"
    elif args.grade:
        pattern = f"{args.grade}*"
        description = f"å¹´çº§ {args.grade}"
    else:
        logger.error("âŒ å¿…é¡»æŒ‡å®š --grade æˆ– --pattern å‚æ•°")
        return 1

    logger.info(f"ğŸš€ å¼€å§‹ä¸º{description}ç”Ÿæˆquests...")
    logger.info(f"ğŸ“ å†…å®¹ç›®å½•: {content_dir}")

    if args.dry_run:
        logger.info("ğŸ” é¢„è§ˆæ¨¡å¼ - ä¸ä¼šä¿®æ”¹å®é™…æ–‡ä»¶")

    generator = GenericQuestGenerator(str(content_dir))

    updated_files, skipped_files = generator.update_modules_by_pattern(pattern, args.dry_run)

    logger.info(f"\nğŸ“Š å¤„ç†å®Œæˆ:")
    logger.info(f"   æ›´æ–°æ–‡ä»¶: {len(updated_files)}")
    logger.info(f"   è·³è¿‡æ–‡ä»¶: {len(skipped_files)}")

    if updated_files:
        logger.info(f"\nâœ… æˆåŠŸ{'é¢„è§ˆ' if args.dry_run else 'æ›´æ–°'}çš„æ–‡ä»¶:")
        for file_name in updated_files:
            logger.info(f"   - {file_name}")

    if skipped_files:
        logger.info(f"\nâš ï¸ è·³è¿‡çš„æ–‡ä»¶:")
        for file_name in skipped_files:
            logger.info(f"   - {file_name}")

    logger.info(f"\nğŸ‰ Questsç”Ÿæˆå®Œæˆ!")
    return 0

if __name__ == "__main__":
    exit(main())